"""
Training data loader — reads pre-exported JSON files.

The JSON files are generated by:
  npm run ml:export   (real data from PostgreSQL)
  npm run ml:generate (synthetic data for development)

File format (see lib/db/scripts/export-ml-data.ts):
  training-data/users.json        — user profiles + tag_weights + stats
  training-data/events.json       — events + attendance stats
  training-data/spaces.json       — spaces + member/event stats
  training-data/interactions.json — positive pairs with recency weights

Triplet format (6-tuple):
  (anchor_type, anchor_vec, item_type, item_vec, label, weight)
  - anchor_type/item_type: "user" | "event" | "space"
  - label: 1 = positive interaction, 0 = sampled negative
  - weight: recency-weighted interaction strength [0.05, 1.0] for positives,
            1.0 for negatives (push all negatives equally hard)
"""

from __future__ import annotations
import json
import os
import random
from collections import defaultdict
from datetime import date, datetime
from typing import Optional

from config import TRAINING_DATA_DIR, NEGATIVE_SAMPLES
from features import (
    build_user_features,
    build_event_features,
    build_space_features,
)


# ─── JSON loaders ──────────────────────────────────────────────────────────────

def _read(data_dir: str, filename: str) -> list[dict]:
    path = os.path.join(data_dir, filename)
    if not os.path.exists(path):
        raise FileNotFoundError(
            f"{path} not found. Run 'npm run ml:export' or 'npm run ml:generate' first."
        )
    with open(path, encoding="utf-8") as f:
        return json.load(f)


# ─── Feature builders from JSON records ────────────────────────────────────────

def _days_until(starts_at: Optional[str]) -> Optional[int]:
    if not starts_at:
        return None
    try:
        dt = datetime.fromisoformat(starts_at)
        return (dt.date() - date.today()).days
    except ValueError:
        return None


def users_to_features(records: list[dict]) -> dict[str, list[float]]:
    result: dict[str, list[float]] = {}
    for u in records:
        result[u["id"]] = build_user_features(
            birthdate=u.get("birthdate"),
            tag_weights=u.get("tag_weights") or {},
            gender=u.get("gender"),
            relationship_intent=u.get("relationship_intent") or [],
            smoking=u.get("smoking"),
            drinking=u.get("drinking"),
            activity_level=u.get("activity_level"),
            interaction_count=int(u.get("interaction_count") or 0),
        )
    return result


def events_to_features(records: list[dict]) -> dict[str, list[float]]:
    result: dict[str, list[float]] = {}
    for e in records:
        result[e["id"]] = build_event_features(
            tags=e.get("tags") or [],
            avg_attendee_age=e.get("avg_attendee_age"),
            attendee_count=int(e.get("attendee_count") or 0),
            days_until_event=_days_until(e.get("starts_at")),
            starts_at=e.get("starts_at"),
            max_attendees=e.get("max_attendees"),
            is_paid=bool(e.get("is_paid")),
            price_cents=(int(e["price_cents"]) if e.get("price_cents") is not None else None),
        )
    return result


def spaces_to_features(records: list[dict]) -> dict[str, list[float]]:
    result: dict[str, list[float]] = {}
    for s in records:
        result[s["id"]] = build_space_features(
            tags=s.get("tags") or [],
            avg_member_age=s.get("avg_member_age"),
            member_count=int(s.get("member_count") or 0),
            event_count=int(s.get("event_count") or 0),
        )
    return result


def _jaccard(a: set[str], b: set[str]) -> float:
    if not a or not b:
        return 0.0
    inter = len(a & b)
    if inter == 0:
        return 0.0
    return inter / len(a | b)


def _build_user_user_edges(
    interactions_raw: list[dict],
    max_users_per_item: int = 80,
    min_shared_items: int = 2,
    top_k_per_user: int = 20,
) -> list[dict]:
    """
    Build explicit user↔user positives from co-engagement.
    Keeps only strong overlap and top-k neighbors per user.
    """
    users_by_item: dict[str, set[str]] = defaultdict(set)
    for r in interactions_raw:
        uid = r.get("user_id")
        iid = r.get("item_id")
        if uid and iid:
            users_by_item[iid].add(uid)

    pair_counts: dict[tuple[str, str], int] = defaultdict(int)
    for users in users_by_item.values():
        ulist = list(users)
        if len(ulist) > max_users_per_item:
            ulist = random.sample(ulist, max_users_per_item)
        ulist.sort()
        for i in range(len(ulist)):
            ui = ulist[i]
            for j in range(i + 1, len(ulist)):
                uj = ulist[j]
                pair_counts[(ui, uj)] += 1

    neighbors: dict[str, list[tuple[str, int]]] = defaultdict(list)
    for (u1, u2), cnt in pair_counts.items():
        if cnt < min_shared_items:
            continue
        neighbors[u1].append((u2, cnt))
        neighbors[u2].append((u1, cnt))

    edges: list[dict] = []
    for uid, peers in neighbors.items():
        peers.sort(key=lambda x: x[1], reverse=True)
        for peer_id, cnt in peers[:top_k_per_user]:
            weight = min(1.0, 0.35 + 0.12 * cnt)
            edges.append({
                "anchor_type": "user",
                "anchor_id": uid,
                "item_type": "user",
                "item_id": peer_id,
                "weight": weight,
            })
    return edges


def _build_same_type_tag_edges(
    records: list[dict],
    entity_type: str,
    min_jaccard: float,
    top_k_per_entity: int,
    max_entities: int = 5_000,
) -> list[dict]:
    """
    Build same-type edges (event↔event, space↔space) from tag overlap.

    Complexity: O(n²) in the number of entities.  ``max_entities`` caps the
    input to keep build time reasonable on large datasets; entities are sampled
    randomly when the cap is exceeded.
    """
    ids = [r["id"] for r in records if r.get("id")]
    if len(ids) > max_entities:
        import warnings
        warnings.warn(
            f"_build_same_type_tag_edges: {len(ids)} {entity_type} entities exceed "
            f"max_entities={max_entities}. Sampling {max_entities} randomly.",
            stacklevel=2,
        )
        ids = random.sample(ids, max_entities)

    tags_by_id = {
        r["id"]: set(r.get("tags") or [])
        for r in records
        if r.get("id")
    }
    neighbors: dict[str, list[tuple[str, float]]] = defaultdict(list)

    for i in range(len(ids)):
        a = ids[i]
        ta = tags_by_id.get(a, set())
        if not ta:
            continue
        for j in range(i + 1, len(ids)):
            b = ids[j]
            tb = tags_by_id.get(b, set())
            if not tb:
                continue
            jac = _jaccard(ta, tb)
            if jac < min_jaccard:
                continue
            neighbors[a].append((b, jac))
            neighbors[b].append((a, jac))

    edges: list[dict] = []
    for eid, sims in neighbors.items():
        sims.sort(key=lambda x: x[1], reverse=True)
        for peer_id, jac in sims[:top_k_per_entity]:
            edges.append({
                "anchor_type": entity_type,
                "anchor_id": eid,
                "item_type": entity_type,
                "item_id": peer_id,
                "weight": min(1.0, 0.2 + jac),
            })
    return edges


# ─── Training data builder ─────────────────────────────────────────────────────

def build_training_data(
    data_dir: str = TRAINING_DATA_DIR,
    val_ratio: float = 0.15,
    negative_samples: int = NEGATIVE_SAMPLES,
) -> dict:
    """
    Loads JSON exports and returns a dict with all data needed for training.

    Returns:
        train_triplets:  list of (anchor_type, anchor_vec, item_type, item_vec, label, weight)
        val_data:        dict for evaluation — {anchor_features, item_features, val_pairs}
        user_features:   {user_id: vec}  — needed for hard neg mining
        item_features:   {item_id: (type, vec)}  — ALL corpus items
        positive_set:    set of (anchor_type, anchor_id, item_type, item_id)  — for negative sampling exclusion

    Val split:
        Anchors with ≥ 3 positive interactions: val_ratio % held out (min 1).
        Anchors with < 3 interactions: all triplets go to training.
    """
    users_raw        = _read(data_dir, "users.json")
    events_raw       = _read(data_dir, "events.json")
    spaces_raw       = _read(data_dir, "spaces.json")
    interactions_raw = _read(data_dir, "interactions.json")

    user_features  = users_to_features(users_raw)
    event_features = events_to_features(events_raw)
    space_features = spaces_to_features(spaces_raw)

    # Full item corpus (all entity types)
    all_item_features: dict[str, tuple[str, list[float]]] = {}
    for uid, fvec in user_features.items():
        all_item_features[uid] = ("user", fvec)
    for eid, fvec in event_features.items():
        all_item_features[eid] = ("event", fvec)
    for sid, fvec in space_features.items():
        all_item_features[sid] = ("space", fvec)

    all_item_ids = list(all_item_features.keys())
    # Pre-split item IDs by type for type-constrained negative sampling.
    # For a positive (anchor_type → item_type), negatives are drawn only from
    # items of the same item_type so the model learns type-specific ranking
    # rather than cross-type discrimination (which dilutes the signal).
    item_ids_by_type: dict[str, list[str]] = {"user": [], "event": [], "space": []}
    for _iid, (_itype, _) in all_item_features.items():
        item_ids_by_type[_itype].append(_iid)

    def _feature_of(entity_type: str, entity_id: str) -> Optional[list[float]]:
        if entity_type == "user":
            return user_features.get(entity_id)
        if entity_type == "event":
            return event_features.get(entity_id)
        if entity_type == "space":
            return space_features.get(entity_id)
        return None

    # ── Build multi-anchor positive records ───────────────────────────────────
    # Base supervision:
    #   user ↔ event/space from interactions
    #   event ↔ space from event.space_id
    # Added explicit same-type links:
    #   user ↔ user from co-engagement
    #   event ↔ event from tag overlap
    #   space ↔ space from tag overlap
    all_positive_records: list[dict] = []

    for r in interactions_raw:
        uid = r["user_id"]
        iid = r["item_id"]
        itype = r["item_type"]
        w = float(r.get("weight") or 1.0)
        all_positive_records.append({
            "anchor_type": "user",
            "anchor_id": uid,
            "item_type": itype,
            "item_id": iid,
            "weight": w,
        })
        all_positive_records.append({
            "anchor_type": itype,
            "anchor_id": iid,
            "item_type": "user",
            "item_id": uid,
            "weight": w,
        })

    for e in events_raw:
        eid = e.get("id")
        sid = e.get("space_id")
        if not eid or not sid:
            continue
        if eid not in event_features or sid not in space_features:
            continue
        all_positive_records.append({
            "anchor_type": "event",
            "anchor_id": eid,
            "item_type": "space",
            "item_id": sid,
            "weight": 1.0,
        })
        all_positive_records.append({
            "anchor_type": "space",
            "anchor_id": sid,
            "item_type": "event",
            "item_id": eid,
            "weight": 1.0,
        })

    # Same-type positives (explicitly requested by product behavior).
    all_positive_records.extend(_build_user_user_edges(interactions_raw))
    all_positive_records.extend(
        _build_same_type_tag_edges(
            # Pre-filter: only pass records that have a feature vector so no
            # orphan edges slip into all_positive_records and get silently dropped.
            records=[r for r in events_raw if r.get("id") in event_features],
            entity_type="event",
            min_jaccard=0.5,
            top_k_per_entity=8,
        )
    )
    all_positive_records.extend(
        _build_same_type_tag_edges(
            records=[r for r in spaces_raw if r.get("id") in space_features],
            entity_type="space",
            min_jaccard=0.45,
            top_k_per_entity=10,
        )
    )

    # Deduplicate directed edges and keep the strongest weight.
    dedup: dict[tuple[str, str, str, str], dict] = {}
    for r in all_positive_records:
        key = (r["anchor_type"], r["anchor_id"], r["item_type"], r["item_id"])
        prev = dedup.get(key)
        if prev is None or float(r["weight"]) > float(prev["weight"]):
            dedup[key] = r
    all_positive_records = list(dedup.values())

    # Full positive set — used to avoid picking positives as negatives.
    # Key is a 4-tuple that includes item_type so we never over-exclude
    # in cases where entity IDs happen to collide across types.
    positive_set = {
        (r["anchor_type"], r["anchor_id"], r["item_type"], r["item_id"])
        for r in all_positive_records
    }

    # ── Train / val split per anchor ──────────────────────────────────────────
    by_anchor: dict[tuple[str, str], list[dict]] = defaultdict(list)
    for r in all_positive_records:
        by_anchor[(r["anchor_type"], r["anchor_id"])].append(r)

    train_records: list[dict] = []
    val_pairs: list[tuple[str, str, str, str]] = []   # (anchor_type, anchor_id, item_type, item_id)
    seen_train_by_anchor: dict[tuple[str, str], set[str]] = defaultdict(set)

    for anchor_key, records in by_anchor.items():
        if len(records) >= 3:
            n_val = max(1, round(len(records) * val_ratio))
            val_recs = random.sample(records, n_val)
            val_set = {id(r) for r in val_recs}
            train_recs = [r for r in records if id(r) not in val_set]
        else:
            val_recs = []
            train_recs = records

        train_records.extend(train_recs)
        for r in train_recs:
            seen_train_by_anchor[anchor_key].add(r["item_id"])
        val_pairs.extend(
            (r["anchor_type"], r["anchor_id"], r["item_type"], r["item_id"])
            for r in val_recs
        )

    # ── Build 6-tuple triplets from train_records ─────────────────────────────
    train_triplets: list[tuple] = []

    for record in train_records:
        anchor_type = record["anchor_type"]
        anchor_id = record["anchor_id"]
        item_type = record["item_type"]
        item_id = record["item_id"]
        weight = float(record.get("weight") or 1.0)

        anchor_vec = _feature_of(anchor_type, anchor_id)
        if anchor_vec is None:
            continue

        item_vec = _feature_of(item_type, item_id)
        if item_vec is None:
            continue

        # Positive triplet — 8-tuple includes entity IDs for loss masking
        train_triplets.append((anchor_type, anchor_id, anchor_vec, item_type, item_id, item_vec, 1, weight))

        # Type-constrained random negatives (weight 1.0).
        # Sample negatives only from items of the same type as the positive item
        # so the loss teaches type-specific ranking (e.g. event→user negatives
        # are other users, not events or spaces which are irrelevant to that task).
        neg_pool = item_ids_by_type.get(item_type, all_item_ids)
        sampled  = 0
        attempts = 0
        while sampled < negative_samples and attempts < negative_samples * 10:
            attempts += 1
            neg_id = random.choice(neg_pool)
            if neg_id == anchor_id:
                continue
            neg_type, neg_vec = all_item_features[neg_id]
            # Check 4-tuple: include neg_type to avoid over-exclusion on ID collisions
            if (anchor_type, anchor_id, neg_type, neg_id) in positive_set:
                continue
            train_triplets.append((anchor_type, anchor_id, anchor_vec, neg_type, neg_id, neg_vec, 0, 1.0))
            sampled += 1

    # ── Bidirectional leakage fix ─────────────────────────────────────────────
    # After the per-anchor split, edges A→B and B→A are in different buckets and
    # may land in different splits: B→A in train while A→B is held out in val.
    # The model then sees the reverse signal during training and the val metric
    # is inflated. Fix: for every canonical pair that has any direction in val,
    # remove the reverse direction from train_records before building triplets.
    #
    # We rebuild the triplet list so both directions are treated as one unit.
    # (The positive_set already covers both directions, so negatives are safe.)
    val_canonical: set[tuple] = set()
    for atype, aid, itype, iid in val_pairs:
        ep_a, ep_b = (atype, aid), (itype, iid)
        val_canonical.add((min(ep_a, ep_b), max(ep_a, ep_b)))

    # Re-filter: rebuild train_triplets without any triplet whose canonical pair
    # is entirely held out in validation.
    train_triplets = [
        t for t in train_triplets
        if (
            min((t[0], t[1]), (t[3], t[4])),
            max((t[0], t[1]), (t[3], t[4])),
        ) not in val_canonical
    ]

    random.shuffle(train_triplets)

    # ── Val data for Recall@K / NDCG@K evaluation ────────────────────────────
    val_anchor_keys = {(atype, aid) for atype, aid, _itype, _iid in val_pairs}
    val_data = {
        "anchor_features": {
            (atype, aid): _feature_of(atype, aid)
            for atype, aid in val_anchor_keys
            if _feature_of(atype, aid) is not None
        },
        "item_features": all_item_features,   # rank against full corpus
        "val_pairs": val_pairs,
        # Exclude train positives during eval to avoid trivial re-recommendations.
        "seen_train_by_anchor": {
            anchor_key: list(items) for anchor_key, items in seen_train_by_anchor.items()
        },
    }

    return {
        "train_triplets":  train_triplets,
        "val_data":        val_data,
        "user_features":   user_features,
        "event_features":  event_features,
        "space_features":  space_features,
        "item_features":   all_item_features,
        "positive_set":    positive_set,
    }


def build_training_triplets(
    data_dir: str = TRAINING_DATA_DIR,
    negative_samples: int = NEGATIVE_SAMPLES,
) -> list[tuple]:
    """
    Backward-compatible wrapper — returns just the training triplets.
    Each triplet is an 8-tuple:
    (anchor_type, anchor_id, anchor_vec, item_type, item_id, item_vec, label, weight).
    """
    result = build_training_data(data_dir=data_dir, negative_samples=negative_samples)
    return result["train_triplets"]
