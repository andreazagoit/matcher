"""
Training data loader — reads pre-exported JSON files.

The JSON files are generated by:
  npm run ml:export   (real data from PostgreSQL)
  npm run ml:generate (synthetic data for development)

File format (see lib/db/scripts/export-ml-data.ts):
  training-data/users.json        — user profiles + tag_weights + stats
  training-data/events.json       — events + attendance stats
  training-data/spaces.json       — spaces + member/event stats
  training-data/interactions.json — positive pairs with recency weights

Triplet format (5-tuple):
  (anchor_vec, item_vec, item_type, label, weight)
  - anchor is always a user (USER_DIM floats)
  - item_type: "user" | "event" | "space"
  - label: 1 = positive interaction, 0 = sampled negative
  - weight: recency-weighted interaction strength [0.05, 1.0] for positives,
            1.0 for negatives (push all negatives equally hard)
"""

from __future__ import annotations
import json
import os
import random
from collections import defaultdict
from datetime import date, datetime
from typing import Optional

from config import TRAINING_DATA_DIR, NEGATIVE_SAMPLES
from features import (
    build_user_features,
    build_event_features,
    build_space_features,
)


# ─── JSON loaders ──────────────────────────────────────────────────────────────

def _read(data_dir: str, filename: str) -> list[dict]:
    path = os.path.join(data_dir, filename)
    if not os.path.exists(path):
        raise FileNotFoundError(
            f"{path} not found. Run 'npm run ml:export' or 'npm run ml:generate' first."
        )
    with open(path, encoding="utf-8") as f:
        return json.load(f)


# ─── Feature builders from JSON records ────────────────────────────────────────

def _days_until(starts_at: Optional[str]) -> Optional[int]:
    if not starts_at:
        return None
    try:
        dt = datetime.fromisoformat(starts_at)
        return (dt.date() - date.today()).days
    except ValueError:
        return None


def users_to_features(records: list[dict]) -> dict[str, list[float]]:
    result: dict[str, list[float]] = {}
    for u in records:
        result[u["id"]] = build_user_features(
            birthdate=u.get("birthdate"),
            tag_weights=u.get("tag_weights") or {},
            gender=u.get("gender"),
            relationship_intent=u.get("relationship_intent") or [],
            smoking=u.get("smoking"),
            drinking=u.get("drinking"),
            activity_level=u.get("activity_level"),
            interaction_count=int(u.get("interaction_count") or 0),
            conversation_count=int(u.get("conversation_count") or 0),
        )
    return result


def events_to_features(records: list[dict]) -> dict[str, list[float]]:
    result: dict[str, list[float]] = {}
    for e in records:
        result[e["id"]] = build_event_features(
            tags=e.get("tags") or [],
            avg_attendee_age=e.get("avg_attendee_age"),
            attendee_count=int(e.get("attendee_count") or 0),
            days_until_event=_days_until(e.get("starts_at")),
            max_attendees=e.get("max_attendees"),
            is_paid=bool(e.get("is_paid")),
        )
    return result


def spaces_to_features(records: list[dict]) -> dict[str, list[float]]:
    result: dict[str, list[float]] = {}
    for s in records:
        result[s["id"]] = build_space_features(
            tags=s.get("tags") or [],
            avg_member_age=s.get("avg_member_age"),
            member_count=int(s.get("member_count") or 0),
            event_count=int(s.get("event_count") or 0),
        )
    return result


# ─── Training data builder ─────────────────────────────────────────────────────

def build_training_data(
    data_dir: str = TRAINING_DATA_DIR,
    val_ratio: float = 0.15,
    negative_samples: int = NEGATIVE_SAMPLES,
) -> dict:
    """
    Loads JSON exports and returns a dict with all data needed for training.

    Returns:
        train_triplets:  list of (anchor_vec, item_vec, item_type, label, weight)
        val_data:        dict for evaluation — {user_features, item_features, val_pairs}
        user_features:   {user_id: vec}  — needed for hard neg mining
        item_features:   {item_id: (type, vec)}  — ALL corpus items
        positive_set:    set of (user_id, item_id)  — for negative sampling exclusion

    Val split:
        Users with ≥ 3 positive interactions: val_ratio % held out (min 1).
        Users with < 3 interactions: all triplets go to training.
    """
    users_raw        = _read(data_dir, "users.json")
    events_raw       = _read(data_dir, "events.json")
    spaces_raw       = _read(data_dir, "spaces.json")
    interactions_raw = _read(data_dir, "interactions.json")

    user_features  = users_to_features(users_raw)
    event_features = events_to_features(events_raw)
    space_features = spaces_to_features(spaces_raw)

    # Full item corpus (all entity types)
    all_item_features: dict[str, tuple[str, list[float]]] = {}
    for uid, fvec in user_features.items():
        all_item_features[uid] = ("user", fvec)
    for eid, fvec in event_features.items():
        all_item_features[eid] = ("event", fvec)
    for sid, fvec in space_features.items():
        all_item_features[sid] = ("space", fvec)

    all_item_ids = list(all_item_features.keys())

    # Full positive set — used to avoid picking positives as negatives
    positive_set = {(r["user_id"], r["item_id"]) for r in interactions_raw}

    # ── Train / val split per user ────────────────────────────────────────────
    by_user: dict[str, list[dict]] = defaultdict(list)
    for r in interactions_raw:
        by_user[r["user_id"]].append(r)

    train_records: list[dict] = []
    val_pairs: list[tuple[str, str]] = []   # (user_id, item_id) held-out positives

    for uid, records in by_user.items():
        if len(records) >= 3:
            n_val     = max(1, round(len(records) * val_ratio))
            val_recs  = random.sample(records, n_val)
            val_set   = {id(r) for r in val_recs}
            train_recs = [r for r in records if id(r) not in val_set]
        else:
            val_recs   = []
            train_recs = records

        train_records.extend(train_recs)
        val_pairs.extend((uid, r["item_id"]) for r in val_recs)

    # ── Build 5-tuple triplets from train_records ─────────────────────────────
    train_triplets: list[tuple] = []

    for record in train_records:
        uid_      = record["user_id"]
        item_id   = record["item_id"]
        item_type = record["item_type"]
        weight    = float(record.get("weight") or 1.0)

        user_vec = user_features.get(uid_)
        if user_vec is None:
            continue

        item_vec = (
            event_features.get(item_id) if item_type == "event"
            else space_features.get(item_id) if item_type == "space"
            else user_features.get(item_id)
        )
        if item_vec is None:
            continue

        # Positive triplet
        train_triplets.append((user_vec, item_vec, item_type, 1, weight))

        # Random negatives (weight 1.0 — push all negatives equally)
        sampled  = 0
        attempts = 0
        while sampled < negative_samples and attempts < negative_samples * 10:
            attempts += 1
            neg_id = random.choice(all_item_ids)
            if (uid_, neg_id) in positive_set or neg_id == uid_:
                continue
            neg_type, neg_vec = all_item_features[neg_id]
            train_triplets.append((user_vec, neg_vec, neg_type, 0, 1.0))
            sampled += 1

    random.shuffle(train_triplets)

    # ── Val data for Recall@K / NDCG@K evaluation ────────────────────────────
    val_user_ids = {uid for uid, _ in val_pairs}
    val_data = {
        "user_features": {
            uid: user_features[uid]
            for uid in val_user_ids
            if uid in user_features
        },
        "item_features": all_item_features,   # rank against full corpus
        "val_pairs": val_pairs,
    }

    return {
        "train_triplets": train_triplets,
        "val_data":       val_data,
        "user_features":  user_features,
        "item_features":  all_item_features,
        "positive_set":   positive_set,
    }


def build_training_triplets(
    data_dir: str = TRAINING_DATA_DIR,
    negative_samples: int = NEGATIVE_SAMPLES,
) -> list[tuple]:
    """
    Backward-compatible wrapper — returns just the training triplets.
    Each triplet is a 5-tuple: (anchor_vec, item_vec, item_type, label, weight).
    """
    result = build_training_data(data_dir=data_dir, negative_samples=negative_samples)
    return result["train_triplets"]
