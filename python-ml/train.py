#!/usr/bin/env python3
"""
Standalone training script.

Usage:
  python3 train.py
  python3 train.py --data-dir ./training-data --epochs 100
  python3 train.py --val-ratio 0.15 --patience 15 --eval-every 5
  python3 train.py --hard-neg-every 20 --n-hard-neg 3

Expects JSON files generated by:
  npm run ml:export    (real data)
  npm run ml:generate  (synthetic data)

Flow:
  1. Load + split data (train / val)
  2. Train with weighted contrastive loss + InfoNCE
  3. Evaluate Recall@10 / NDCG@10 every --eval-every epochs
  4. Early stop if val metric doesn't improve for --patience epochs
  5. Optionally refresh hard negatives every --hard-neg-every epochs
  6. Save best checkpoint to model_weights.pt
"""

from __future__ import annotations
import argparse
import os
import sys
import time

from config import (
    TRAINING_DATA_DIR,
    EPOCHS,
    BATCH_SIZE,
    NEGATIVE_SAMPLES,
    MODEL_WEIGHTS_PATH,
)
from data import build_training_data
from model import train, save_model, mine_hard_negatives


def main() -> None:
    parser = argparse.ArgumentParser(
        description="Train the HGT-lite model.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )

    # Data
    parser.add_argument("--data-dir",      default=TRAINING_DATA_DIR,
                        help="Directory with JSON export files")
    parser.add_argument("--neg-samples",   type=int, default=NEGATIVE_SAMPLES,
                        help="Random negatives per positive interaction")
    parser.add_argument("--val-ratio",     type=float, default=0.15,
                        help="Fraction of each user's interactions held out for validation")

    # Training
    parser.add_argument("--epochs",        type=int, default=EPOCHS)
    parser.add_argument("--batch-size",    type=int, default=BATCH_SIZE)
    parser.add_argument("--patience",      type=int, default=15,
                        help="Early stopping patience in epochs (0 = disabled)")
    parser.add_argument("--eval-every",    type=int, default=1,
                        help="Compute val Recall@10 / NDCG@10 every N epochs (0 = disabled)")
    parser.add_argument("--checkpoint-every", type=int, default=1,
                        help="Save recovery checkpoint every N epochs (0 = disabled)")

    # Hard negative mining
    parser.add_argument("--hard-neg-every", type=int, default=0,
                        help="Refresh hard negatives every N epochs (0 = disabled)")
    parser.add_argument("--n-hard-neg",     type=int, default=3,
                        help="Hard negatives per user per refresh")

    args = parser.parse_args()

    print("=" * 60)
    print("  HGT-lite training")
    print("=" * 60)
    print(f"  data-dir       : {args.data_dir}")
    print(f"  neg-samples    : {args.neg_samples}")
    print(f"  val-ratio      : {args.val_ratio:.0%}")
    print(f"  epochs         : {args.epochs}")
    print(f"  batch-size     : {args.batch_size}")
    print(f"  patience       : {args.patience or 'disabled'}")
    print(f"  eval-every     : {args.eval_every or 'disabled'}")
    print(f"  checkpoint-ev. : {args.checkpoint_every or 'disabled'}")
    print(f"  hard-neg-every : {args.hard_neg_every or 'disabled'}")
    if args.hard_neg_every > 0:
        print(f"  n-hard-neg     : {args.n_hard_neg}")
    print()

    # ── Load data ──────────────────────────────────────────────────────────────
    print("Loading data...", flush=True)
    t0 = time.time()
    try:
        result = build_training_data(
            data_dir=args.data_dir,
            val_ratio=args.val_ratio,
            negative_samples=args.neg_samples,
        )
    except FileNotFoundError as e:
        print(f"\nError: {e}", file=sys.stderr)
        sys.exit(1)

    train_triplets = result["train_triplets"]
    val_data       = result["val_data"]
    user_features  = result["user_features"]
    item_features  = result["item_features"]
    positive_set   = result["positive_set"]

    if not train_triplets:
        print("No training data found. Exiting.")
        sys.exit(0)

    pos   = sum(1 for *_, label, _w in train_triplets if label == 1)
    neg   = len(train_triplets) - pos
    n_val = len(val_data["val_pairs"])
    n_val_users = len(val_data["user_features"])

    print(f"  Train triplets : {len(train_triplets):>10,}  ({pos:,} pos / {neg:,} neg)")
    print(f"  Val pairs      : {n_val:>10,}  from {n_val_users:,} users")
    print(f"  Corpus items   : {len(item_features):>10,}")
    print(f"  Loaded in {time.time() - t0:.1f}s\n")

    # ── Hard negative closure ──────────────────────────────────────────────────
    hard_neg_fn = None
    if args.hard_neg_every > 0:
        def hard_neg_fn(model):
            return mine_hard_negatives(
                model=model,
                user_features=user_features,
                item_features=item_features,
                positive_set=positive_set,
                n_per_user=args.n_hard_neg,
            )

    # ── Train ──────────────────────────────────────────────────────────────────
    print("Training...\n", flush=True)
    if os.path.exists(MODEL_WEIGHTS_PATH):
        print(f"Resuming from existing checkpoint: {MODEL_WEIGHTS_PATH}", flush=True)
    else:
        print("No checkpoint found, training from scratch.", flush=True)
    t1 = time.time()
    model = train(
        triplets=train_triplets,
        val_data=val_data if args.eval_every > 0 else None,
        epochs=args.epochs,
        batch_size=args.batch_size,
        patience=args.patience,
        eval_every=args.eval_every,
        checkpoint_every=args.checkpoint_every,
        hard_neg_fn=hard_neg_fn,
        hard_neg_every=args.hard_neg_every,
    )
    elapsed = time.time() - t1
    print(f"\nTraining completed in {elapsed:.0f}s")

    # ── Final evaluation ───────────────────────────────────────────────────────
    if val_data["val_pairs"]:
        from model import evaluate_recall_ndcg
        metrics = evaluate_recall_ndcg(model, val_data, k=10, max_users=1000)
        print(f"\nFinal metrics:")
        print(f"  Recall@10  : {metrics['recall@k']:.4f}")
        print(f"  NDCG@10    : {metrics['ndcg@k']:.4f}")
        print(f"  (evaluated on {metrics['n_users']} users)")

    save_model(model)
    print("\nDone. Run 'npm run ml:embed-all' to regenerate entity embeddings.")


if __name__ == "__main__":
    main()
